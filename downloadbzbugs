#!/usr/bin/env python3

"""Download backup XML from bugzilla

Usage:
  downloadbzbugs <product/component> <outdir> [--browser=BROWSER --base-url=URL]

Options:
  --base-url=URL     Base URL of the Bugzilla instance [default: https://bugzilla.gnome.org].
  --browser=BROWSER  Name of the browser where to find cookie to connect to bugzilla [default: firefox3].

  -h --help          Show this screen.
  --version          Show version.
"""
import os
import re
import time
import sys
import shutil

from urllib.parse import urlparse
from docopt import docopt
from configparser import RawConfigParser
import xml.etree.ElementTree as ET
import csv
import requests
import tempfile
import glob
import base64

try:
    from sqlite3 import dbapi2 as sqlite
except ImportError:
    from pysqlite2 import dbapi2 as sqlite

# All cookie retrieval related code was taken from git-bz: http://git.fishsoup.net/cgit/git-bz/
class CookieError(Exception):
    pass

def die(message):
    print(message)
    sys.exit(1)


def do_get_cookies_from_sqlite(host, cookies_sqlite, browser, query, chromium_time):
    result = {}
    # We use a timeout of 0 since we expect to hit the browser holding
    # the lock often and we need to fall back to making a copy without a delay
    connection = sqlite.connect(cookies_sqlite, timeout=0)

    try:
        cursor = connection.cursor()
        cursor.execute(query, {'host': host})

        now = time.time()
        for name, value, path, expiry in cursor.fetchall():
            # Excessive caution: toss out values that need to be quoted in a cookie header
            expiry = float(expiry)
            if chromium_time:
                # Time stored in microseconds since epoch
                expiry /= 1000000.
                # Old chromium versions used to use the Unix epoch, but newer versions
                # use the Windows epoch of January 1, 1601. Convert the latter to Unix epoch
                if expiry > 11644473600:
                    expiry -= 11644473600
            if float(expiry) > now and not re.search(r'[()<>@,;:\\"/\[\]?={} \t]', value):
                result[name] = value

        return result
    finally:
        connection.close()

# Firefox 3.5 keeps the cookies database permamently locked; as a workaround
# hack, we make a copy, read from that, then delete the copy. Of course,
# we may hit an inconsistent state of the database
def get_cookies_from_sqlite_with_copy(host, cookies_sqlite, browser, *args, **kwargs):
    db_copy = cookies_sqlite + ".git-bz-temp"
    shutil.copyfile(cookies_sqlite, db_copy)
    try:
        return do_get_cookies_from_sqlite(host, db_copy, browser, *args, **kwargs)
    except sqlite.OperationalError as e:
        raise CookieError("Cookie database was locked; temporary copy didn't work %s" % e)
    finally:
        os.remove(db_copy)

def get_cookies_from_sqlite(host, cookies_sqlite, browser, query, chromium_time=False):
    try:
        result = do_get_cookies_from_sqlite(host, cookies_sqlite, browser, query,
                                            chromium_time=chromium_time)
    except sqlite.OperationalError as e:
        if "database is locked" in str(e):
            # Try making a temporary copy
            result = get_cookies_from_sqlite_with_copy(host, cookies_sqlite, browser, query,
                                                       chromium_time=chromium_time)
        else:
            raise

    if not ('Bugzilla_login' in result and 'Bugzilla_logincookie' in result):
        raise CookieError("You don't appear to be signed into %s; please log in with %s" % (host,
                                                                                            browser))

    return result

def get_cookies_from_sqlite_xulrunner(host, cookies_sqlite, name):
    return get_cookies_from_sqlite(host, cookies_sqlite, name,
                                   "select name,value,path,expiry from moz_cookies where host in (:host, '.'||:host)")

def get_bugzilla_cookies_ff3(host):
    if os.uname()[0] == 'Darwin':
        profiles_dir = os.path.expanduser('~/Library/Application Support/Firefox')
    else:
        profiles_dir = os.path.expanduser('~/.mozilla/firefox')
    profile_path = None

    cp = RawConfigParser()
    cp.read(os.path.join(profiles_dir, "profiles.ini"))
    for section in cp.sections():
        if not cp.has_option(section, "Path"):
            continue

        if (not profile_path or (cp.has_option(section, "Default") and cp.get(section, "Default").strip() == "1")):
            profile_path = os.path.join(profiles_dir, cp.get(section, "Path").strip())

    if not profile_path:
        raise CookieError("Cannot find default Firefox profile")

    cookies_sqlite = os.path.join(profile_path, "cookies.sqlite")
    if not os.path.exists(cookies_sqlite):
        raise CookieError("%s doesn't exist." % cookies_sqlite)

    return get_cookies_from_sqlite_xulrunner(host, cookies_sqlite, "Firefox")

def get_bugzilla_cookies_galeon(host):
    cookies_sqlite = os.path.expanduser('~/.galeon/mozilla/galeon/cookies.sqlite')
    if not os.path.exists(cookies_sqlite):
        raise CookieError("%s doesn't exist." % cookies_sqlite)

    return get_cookies_from_sqlite_xulrunner(host, cookies_sqlite, "Galeon")

def get_bugzilla_cookies_epy(host):
    # epiphany-webkit migrated the cookie db to a different location, but the
    # format is the same
    profile_dir = os.path.expanduser('~/.config/epiphany')
    cookies_sqlite = os.path.join(profile_dir, "cookies.sqlite")
    if not os.path.exists(cookies_sqlite):
        # try pre-GNOME-3.6 location
        profile_dir = os.path.expanduser('~/.gnome2/epiphany')
        cookies_sqlite = os.path.join(profile_dir, "cookies.sqlite")
        if not os.path.exists(cookies_sqlite):
            # try the old location
            cookies_sqlite = os.path.join(profile_dir, "mozilla/epiphany/cookies.sqlite")

    if not os.path.exists(cookies_sqlite):
        raise CookieError("%s doesn't exist" % cookies_sqlite)

    return get_cookies_from_sqlite_xulrunner(host, cookies_sqlite, "Epiphany")

# Shared for Chromium and Google Chrome
def get_bugzilla_cookies_chr(host, browser, config_dir):
    config_dir = os.path.expanduser(config_dir)
    cookies_sqlite = os.path.join(config_dir, "Cookies")
    if not os.path.exists(cookies_sqlite):
        raise CookieError("%s doesn't exist" % cookies_sqlite)
    return get_cookies_from_sqlite(host, cookies_sqlite, browser,
                                   "select name,value,path,expires_utc from cookies where host_key in (:host, '.'||:host)",
                                   chromium_time=True)

def get_bugzilla_cookies_chromium(host):
    if os.uname()[0] == 'Darwin':
        config_dir = '~/Library/Application Support/Chromium/Default'
    else:
        config_dir = '~/.config/chromium/Default'
    return get_bugzilla_cookies_chr(host,
                                    "Chromium",
                                    config_dir)

def get_bugzilla_cookies_google_chrome(host):
    if os.uname()[0] == 'Darwin':
        config_dir = '~/Library/Application Support/Google/Chrome/Default'
    else:
        config_dir = '~/.config/google-chrome/Default'
    return get_bugzilla_cookies_chr(host,
                                    "Google Chrome",
                                    config_dir)

browsers = {'firefox3': get_bugzilla_cookies_ff3,
            'epiphany': get_bugzilla_cookies_epy,
            'galeon': get_bugzilla_cookies_galeon,
            'chromium': get_bugzilla_cookies_chromium,
            'google-chrome': get_bugzilla_cookies_google_chrome}

def browser_list():
    return ", ".join(sorted(browsers.keys()))

def get_bugzilla_cookies(host, browser):
    if browser in browsers:
        do_get_cookies = browsers[browser]
    else:
        die('Unsupported browser %s (we only support %s)' % (browser, browser_list()))

    try:
        return do_get_cookies(host)
    except CookieError as e:
        die("""Error getting login cookie from browser: %s
Configured browser: %s
Possible browsers: %s""" %
            (e, browser, browser_list()))

def response_stream_to_file(r, f, decode_unicode=False):
    total = 0
    print(r.url, end="", flush=True)
    for chunk in r.iter_content(chunk_size=1024, decode_unicode=decode_unicode):
        total += len(chunk)
        print("\r\033[K", r.url, " downloaded ", total, " bytes", sep="", end="", flush=True)
        f.write(chunk)
    print()

class Downloader:
    def __init__(self, base_url, cookies, outdir):
        self.base_url = base_url
        self.cookies = cookies
        self.outdir = outdir

    def setup(self):
        os.makedirs(self.outdir, exist_ok=True)
        self.attachmentsdir = os.path.join(self.outdir, "attachments")
        os.makedirs(self.attachmentsdir, exist_ok=True)

    def bug_list(self, product, component):
        params = { "offset": 0, "product": product, "ctype": "csv" }
        if component:
            params["component"] = component

        bug_ids = []
        while True:
            # we can't be sure that we got all the bugs or we just hit the rows
            # limit configured for the current bugzilla instance, so try again
            # until we get an empty list
            print("listing bugs at offset", params['offset'])
            print(self.base_url + "/buglist.cgi")
            r = requests.get(self.base_url + "/buglist.cgi", params=params, cookies=self.cookies)

            reader = csv.DictReader(r.text.splitlines())
            ids = [row["bug_id"] for row in reader]
            if not ids:
                break
            bug_ids += ids
            params["offset"] += len(ids)
        bug_ids.sort()
        return bug_ids

    def bug_skip(self):
        bug_ids = []
        for bugsfile in glob.iglob(os.path.join(self.outdir, "bugs.*.xml")):
            root = ET.parse(bugsfile).getroot()
            bug_ids += (b.text for b in root.findall("bug/bug_id"))
        return bug_ids

    def bug_fetch(self, bug_ids):
        bugsfiles = glob.glob(os.path.join(self.outdir, "bugs.*.xml"))
        batch = 1000
        for offset in range(0, len(bug_ids), batch):
            ids = bug_ids[offset:offset+batch]
            print("fetching", len(ids), "bugs at offset", offset, "of", len(bug_ids))
            print(self.base_url + "/show_bug.cgi")

            r = requests.post(self.base_url + "/show_bug.cgi",
                    cookies=self.cookies,
                    data={"ctype": "xml", "id": ids},
                    stream=True
                )
            idx = len(bugsfiles) + 1
            with tempfile.NamedTemporaryFile("w", dir=self.outdir) as f:
                response_stream_to_file(r, f, decode_unicode=True)
                bugsfile = os.path.join(self.outdir, "bugs.%04d.xml" % idx)
                os.link(f.name, bugsfile)
                bugsfiles.append(bugsfile)
        return bugsfiles

    def attachment_list(self, bugsfiles):
        attach_ids = []
        for bugsfile in bugsfiles:
            root = ET.parse(bugsfile).getroot()
            attach_ids += (a.findtext("attachid") for a in root.findall("*/attachment") if a.find("data") is None)
        return attach_ids

    def attachment_extract(self, bugsfiles):
        attach_ids = []
        for bugsfile in bugsfiles:
            et = ET.parse(bugsfile)
            root = et.getroot()
            attachments = [a for a in root.findall("*/attachment") if a.find("data") is not None]
            for count, a in enumerate(attachments):
                print("extracting attachment", count+1, "of", len(attachments), "from", bugsfile)
                data = a.find("data")
                assert data.get('encoding') == 'base64'
                attachid = a.findtext("attachid")
                attachfile = os.path.join(self.attachmentsdir, "attach.%s.dat" % attachid)
                if os.path.exists(attachfile):
                    continue
                raw = base64.b64decode(data.text)
                with tempfile.NamedTemporaryFile("wb", dir=self.attachmentsdir) as f:
                    f.write(raw)
                    os.link(f.name, attachfile)
                a.remove(data)
                attach_ids.append(attachid)
            with tempfile.NamedTemporaryFile("wb", dir=self.outdir, delete=False) as f:
                et.write(f, encoding='utf-8', xml_declaration=True)
                os.replace(f.name, bugsfile)
        return attach_ids

    def attachment_skip(self):
        ids = [os.path.basename(f).lstrip('attach.').rstrip('.dat') for f in glob.iglob(os.path.join(self.attachmentsdir, "attach.*.dat"))]
        return ids

    def attachment_fetch(self, attach_ids):
        attachfiles = []
        for count, attachid in enumerate(attach_ids):
            print("fetching attachment", count, "of", len(attach_ids))
            print(self.base_url + "/attachment.cgi")
            r = requests.get(self.base_url + "/attachment.cgi",
                    params={"id": attachid},
                    cookies=self.cookies,
                    stream=True)
            with tempfile.NamedTemporaryFile("wb", dir=self.attachmentsdir) as f:
                response_stream_to_file(r, f)
                attachfile = os.path.join(self.attachmentsdir, "attach.%s.dat" % attachid)
                os.link(f.name, attachfile)
                attachfiles.append(attachfile)
        return attachfiles

def main(arguments):
    product_component = arguments["<product/component>"].split("/")

    product = product_component[0]
    component = product_component[1] if len(product_component) > 1 else None

    url = arguments.get('--base-url')
    cookies = get_bugzilla_cookies(urlparse(url).netloc, arguments.get("--browser"))

    downloader = Downloader(url, cookies, arguments.get('<outdir>'))
    downloader.setup()
    bug_ids = downloader.bug_list(product, component)
    bug_skip = downloader.bug_skip()
    bug_ids = list(sorted(set(bug_ids) - set(bug_skip)))
    bugsfiles = downloader.bug_fetch(bug_ids)
    attach_ids = downloader.attachment_list(bugsfiles)
    downloader.attachment_extract(bugsfiles)
    attach_skip = downloader.attachment_skip()
    attach_ids = list(sorted(set(attach_ids) - set(attach_skip)))
    downloader.attachment_fetch(attach_ids)

if __name__ == '__main__':
    arguments = docopt(__doc__, version='downloadbzbugs 0.1')
    main(arguments)
